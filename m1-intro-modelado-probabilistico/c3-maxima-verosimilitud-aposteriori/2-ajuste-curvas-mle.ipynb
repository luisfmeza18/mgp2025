{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajuste de curvas por principio de máxima verosimilitud.\n",
    "\n",
    "![BLR](https://upload.wikimedia.org/wikipedia/commons/e/ed/Residuals_for_Linear_Regression_Fit.png)\n",
    "\n",
    "> **Objetivos:**\n",
    "> - Recordar el ajuste de curvas polinomiales.\n",
    "> - Entender el fenómeno de overfitting en casos prácticos.\n",
    "> - Explicar los mínimos cuadrados ordinaros mediante el principio de máxima verosimilitud.\n",
    "\n",
    "> **Referencias:**\n",
    "> \n",
    "> - Pattern Recognition and Machine Learning, by Christopher M. Bishop - Cap. 3.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introducción\n",
    "\n",
    "Suponga que tenemos un conjunto de entrenamiento con $N$ observaciones de $x$, \n",
    "\n",
    "$$[x_1, \\dots, x_N],$$\n",
    "\n",
    "en conjunto con las observaciones correspondientes de la variable objetivo $y$, \n",
    "\n",
    "$$[y_1, \\dots, y_N].$$\n",
    "\n",
    "En la siguiente gráfica mostramos datos de entrenamiento, con $N=20$. Estos datos se generaron eligiendo $x$ uniformemente espaciados en el intervalo $[0, 1]$, y la variable objetivo $y$ como el resultado de la función $\\sin (2 \\pi x)$ más un pequeño ruido distribuido normal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos librerías\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos datos ficticios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfica de los datos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro objetivo es explotar estos datos de entrenamiento para hacer predicciones $\\hat{y}$ de la variable objetivo para algún nuevo valor de la variable de entrada.\n",
    "\n",
    "Esta es una tarea compleja, que implica \"descubrir\" la función subyacente $\\sin(2 \\pi x)$ a partir de un conjunto finito de datos. Además, los datos observados están corruptos con ruido, haciendo que el valor de $y$ tenga incertidumbre. Como ya vimos, la teoría de probabilidad nos da un marco de trabajo para expresar dicha incertidumbre de una forma precisa y cuantitativa.\n",
    "\n",
    "Antes de formular el problema de forma probabilística, procedamos \"informalmente\" y de forma más intuitiva. Lo que queremos hacer es ajustar a los datos una función polinomial de la forma:\n",
    "\n",
    "$$\n",
    "f(x, w) = w_0 + w_1 x + w_2 x^{2} + \\dots + w_M x^{M} = \\sum_{j=0}^{M} w_j x^j.\n",
    "$$\n",
    "\n",
    "Notamos que aunque $f$ es una función no lineal de $x$, es una función **lineal respecto a los coeficientes $w$**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los valores de los coeficientes serán determinados ajustando el polinomio a los datos de entrenamiento. Esto se puede lograr minimizando una **función de error** que mide la falta de ajuste entre la función $f(x, w)$ y los datos de entrenamiento.\n",
    "\n",
    "Una elección comúnmente usada para esta función de error está dada por la suma de cuadrados de los errores entre las predicciones sobre los datos de entrenamiento $f(x_n,w)$ y los valores correspondientes del objetivo $y_n$, de forma que minimizaremos:\n",
    "\n",
    "$$\n",
    "E(w) = \\frac{1}{2}\\sum_{n=1}^{N}\\left(f(x_n, w) - y_n\\right)^2.\n",
    "$$\n",
    "\n",
    "La intuición detrás de esta función de error es que siempre es no-negativa, y es cero si y solo si la función $f(x, w)$ pasara exactamente por todos los puntos de entrenamiento. La interpretación geométrica de la función de error se encuentra en la figura del encabezado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De forma que podemos resolver el problema de ajuste de curvas mediante la elección de $w$ para la cual $E(w)$ sea lo más pequeña posible.\n",
    "\n",
    "**Nota.** Dado que $E(w)$ es una función cuadrática de los coeficientes $w$, sus derivadas respecto a los coeficientes serán lineales respecto a $w$, y el problema de minimización tendrá solución única."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expresión matricial de la función objetivo\n",
    "\n",
    "Antes de continuar, conviene obtener una representación más compacta del problema anterior. Comencemos por trabajar con el polinomio, dándonos cuenta de que este es un producto punto entre los coeficientes $w$ y las potencias de $x$:\n",
    "\n",
    "$$\n",
    "f(x, w) = w_0 + w_1 x + w_2 x^{2} + \\dots + w_M x^{M} = \\underbrace{[1 \\quad x \\quad x^2 \\quad \\dots \\quad x^M]}_{\\phi(x)^T} \\underbrace{\\left[\\begin{array}{c} w_0 \\\\ w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_M\\end{array}\\right]}_{w} = \\phi(x)^T w,\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, por otra parte, recordemos la definición de la norma euclidiana de un vector. Si tenemos un vector \n",
    "\n",
    "$$\n",
    "v = \\left[\\begin{array}{c} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_s\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "la norma euclidiana de $v$ es\n",
    "\n",
    "$$\n",
    "||v|| = \\sqrt{v_1^2 + v_2^2 + \\dots + v_s^2},\n",
    "$$\n",
    "\n",
    "o equivalentemente $||v||^2 = v_1^2 + v_2^2 + \\dots + v_s^2 = \\sum_{i=1}^{s} v_i^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con lo anterior, notemos que la función de error la podemos reescribir en términos de la norma de un vector:\n",
    "\n",
    "$$\n",
    "E(w) = \\frac{1}{2}\\sum_{n=1}^{N}\\left(f(x_n, w) - y_n\\right)^2 = \\frac{1}{2}\\sum_{n=1}^{N}\\left(\\phi(x_n)^T w - y_n\\right)^2 = \\frac{1}{2}||\\alpha||^2,\n",
    "$$\n",
    "\n",
    "donde el vector $\\alpha$ es:\n",
    "\n",
    "$$\n",
    "\\begin{align}\\nonumber\n",
    "\\alpha & = & \\left[\\begin{array}{c} \\phi(x_1)^T w - y_1 \\\\ \\phi(x_2)^T w - y_2 \\\\ \\vdots \\\\ \\phi(x_N)^T w - y_N\\end{array}\\right] \\\\ \\nonumber\n",
    "       & = & \\underbrace{\\left[\\begin{array}{ccc} - & \\phi(x_1)^T & - \\\\ - & \\phi(x_2)^T & - \\\\ & \\vdots & \\\\ - & \\phi(x_N)^T & -\\end{array}\\right]}_{\\Phi} w - \\underbrace{\\left[\\begin{array}{c} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N\\end{array}\\right]}_{y} \\\\ \\nonumber\n",
    "       & = & \\Phi w - y\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De este modo, queremos encontrar\n",
    "\n",
    "$$\n",
    "\\hat{w} = \\arg \\min_{w} \\frac{1}{2} ||\\Phi w - y||^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ¿Cómo hacemos esto con Scikit-Learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajuste de curvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn.model_selection.train_test_split\n",
    "\n",
    "# sklearn.preprocessing.PolynomialFeatures\n",
    "\n",
    "# sklearn.linear_model.LinearRegression\n",
    "\n",
    "# sklearn.preprocessing.StandardScaler\n",
    "\n",
    "# sklearn.pipeline.Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicar PolynomialFeatures y su relación con Phi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grado 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición del modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train/test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajuste del modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coeficientes de la regresión lineal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score sobre datos de entrenamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score sobre datos de prueba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grado 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición del modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajuste del modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score sobre datos de entrenamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score sobre datos de prueba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los parámetros de modelo se están sobreajustando a los datos de entrenamiento. Lo podemos observar a través del score sobre los datos de entrenamiento, el cual es muy alto.\n",
    "\n",
    "Sin embargo, este modelo (sobreajustado) tiene un poder de generalización (poder de predicción sobre datos no vistos) bastante malo, lo cual podemos ver en el score sobre los datos de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coeficientes del modelo de grado 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coeficientes del modelo de grado 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo, mientras más complejo, hace un gran esfuerzo por ajustarse a los datos entrenamiento. Esto lo podemos ver en el valor de los parámetros encontrados.\n",
    "\n",
    "Ante un dato no visto, el modelo va a generalizar de forma muy pobre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cómo lidiar con el overfitting? \n",
    "\n",
    "#### 1. Regularización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar sklearn.linear_model.Ridge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición del modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajuste del modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score sobre datos de entrenamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score sobre datos de prueba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coeficientes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Más datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos datos ficticios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfica de los datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train/test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición del modelo - Modelo de grado 10 sin regularización\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajuste del modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score sobre datos de entrenamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score sobre datos de prueba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siempre incluir términos de regularización\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Desde una perspectiva probabilística\n",
    "\n",
    "Para modelar la incertidumbre en este tipo de relaciones, podemos suponer que el ruido aditivo sigue una densidad Gaussiana:\n",
    "\n",
    "$$\n",
    "y = \\phi(x)^T w + \\epsilon,\n",
    "$$\n",
    "\n",
    "con $\\epsilon \\sim \\mathcal{N}(0, \\beta^{-1})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De manera que\n",
    "\n",
    "$$\n",
    "p(y | x, w) = \\mathcal{N}(y | \\phi(x)^T w, \\beta^{-1}),\n",
    "$$\n",
    "\n",
    "es decir, con la relación $\\phi(x)^T w$ modela el valor esperado de la variable de salida $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimación de parámetros: Máxima verosimilitud\n",
    "\n",
    "Para estimar los parámetros, escribimos entonces la función de verosimilitud:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(w) = p(y | X, w) = \\prod_{i=1}^{N} \\mathcal{N}(y_i | \\phi(x_i)^T w, \\beta^{-1}).\n",
    "$$\n",
    "\n",
    "Por tanto, la log verosimilitud es:\n",
    "\n",
    "\\begin{align}\n",
    "l(w) & = \\log \\prod_{i=1}^{N} \\mathcal{N}(y_i | \\phi(x_i)^T w, \\beta^{-1}) \\\\\n",
    "     & = \\sum_{i=1}^{N} \\log\\mathcal{N}(y_i | \\phi(x_i)^T w, \\beta^{-1}) \\\\\n",
    "     & = \\frac{N}{2}\\log\\beta - \\frac{N}{2}\\log(2 \\pi) - \\frac{\\beta}{2} \\sum_{i=1}^{N} (y_i - \\phi(x_i)^T w)^2 \\\\\n",
    "     & = \\frac{N}{2}\\log\\beta - \\frac{N}{2}\\log(2 \\pi) - \\frac{\\beta}{2} \\left|\\left|y - \\Phi w\\right|\\right|^2,\n",
    "\\end{align}\n",
    "\n",
    "donde:\n",
    "\n",
    "$$\n",
    "\\Phi = \\left[\n",
    "    \\begin{array}{ccc}\n",
    "    - & \\phi(x_1)^T  & - \\\\\n",
    "    - & \\phi(x_2)^T  & - \\\\\n",
    "      & \\vdots       &   \\\\\n",
    "    - & \\phi(x_N)^T  & - \\\\\n",
    "    \\end{array}\n",
    "\\right] \\in \\mathbb{R}^{N \\times d}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta manera, usando el principio de máxima verosimilitud, obtenemos que:\n",
    "\n",
    "$$\n",
    "\\hat{w}_{MLE} = \\arg \\max_{w} l(w) = \\arg \\min_{w} \\left|\\left|y - \\Phi w\\right|\\right|^2,\n",
    "$$\n",
    "\n",
    "justo como en mínimos cuadrados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que la estimación de parámetros por máxima verosimilitud, explica nuestra intuición detrás de mínimos cuadrados.\n",
    "\n",
    "Además, **una vez más concluimos que el enfoque de máxima verosimilitud nos puede traer problemas de overfitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script>\n",
    "  $(document).ready(function(){\n",
    "    $('div.prompt').hide();\n",
    "    $('div.back-to-top').hide();\n",
    "    $('nav#menubar').hide();\n",
    "    $('.breadcrumb').hide();\n",
    "    $('.hidden-print').hide();\n",
    "  });\n",
    "</script>\n",
    "\n",
    "<footer id=\"attribution\" style=\"float:right; color:#808080; background:#fff;\">\n",
    "Created with Jupyter by Esteban Jiménez Rodríguez.\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mebo2024_v4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
